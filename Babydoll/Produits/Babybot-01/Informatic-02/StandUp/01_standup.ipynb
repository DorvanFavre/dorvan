{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "import standup_env\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "from stable_baselines3 import PPO, DQN, A2C, DDPG\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "import shutil\n",
    "from typing import Callable\n",
    "\n",
    "name = 'standup_agent'\n",
    "env_name = \"standup_env/StandUp-v1\"\n",
    "max_episode_steps = 200\n",
    "\n",
    "agent = PPO\n",
    "policy = 'MlpPolicy'\n",
    "dir = f\"./{name}\"\n",
    "tensorboard_log = f\"./{name}/t_logs/\"\n",
    "best_model_path = f\"./{name}/model/best_model.zip\"\n",
    "model_path = f\"./{name}/model/model.zip\"\n",
    "best_model_save_path = f\"./{name}/model/\"\n",
    "log_path = f\"./{name}/logs/\"\n",
    "device = 'cpu'\n",
    "\n",
    "# num_action = 20\n",
    "# keys = [i for i in range(num_action)]\n",
    "# values = [(i/num_action)*2 - 1 for i in range(num_action)]\n",
    "# disc_to_cont = dict(zip(keys, values))\n",
    "def make_env(render_mode = None):\n",
    "    env = gym.make(\n",
    "        env_name,\n",
    "        render_mode = render_mode,\n",
    "        skip_frame = 10,\n",
    "        max_episode_steps=max_episode_steps,\n",
    "        debug_mode = False\n",
    "        )\n",
    "    \n",
    "    #env = DiscreteActions(env, disc_to_cont)\n",
    "    #env = TimeAwareObservation(env, normalize_time=True)\n",
    "    return env\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render human\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dorva\\Documents\\GitHub\\dorvan\\Babydoll\\Produits\\Babybot-01\\Informatic-02\\StandUp\\venv2\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:134: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\dorva\\Documents\\GitHub\\dorvan\\Babydoll\\Produits\\Babybot-01\\Informatic-02\\StandUp\\venv2\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "c:\\Users\\dorva\\Documents\\GitHub\\dorvan\\Babydoll\\Produits\\Babybot-01\\Informatic-02\\StandUp\\venv2\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:134: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\dorva\\Documents\\GitHub\\dorvan\\Babydoll\\Produits\\Babybot-01\\Informatic-02\\StandUp\\venv2\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "Action: [-0.83568066 -0.05639767  0.9413816   0.05706795 -0.9350656  -0.30451325\n",
      "  0.7901161  -0.5145137  -0.5306194  -0.260115   -0.81540984 -0.7905143 ]\n",
      "Observations: [ 0.    0.   -1.    0.    0.   -1.    0.    0.   -1.    0.    0.   -1.\n",
      "  0.15  0.  ]\n",
      "Reward: -0.125\n",
      "Info {'reward:': -0.125, 'angle_reward': 0.5, 'z_position_reward': 0.375}\n",
      "Step: 1\n",
      "Action: [ 0.4408463  -0.33787316  0.18298551  0.9845838  -0.04923519  0.2866405\n",
      " -0.41073093 -0.1175947   0.22800392 -0.6251257   0.61446273  0.55848175]\n",
      "Observations: [-0.08356807 -0.00563977 -0.90586184  0.00570679 -0.09350656 -1.\n",
      "  0.07901161 -0.05145137 -1.         -0.0260115  -0.08154098 -1.\n",
      "  0.02639597  0.00203186]\n",
      "Reward: -0.44416937323560024\n",
      "Info {'reward:': -0.44416937323560024, 'angle_reward': 0.4898406911420201, 'z_position_reward': 0.06598993562237959}\n",
      "Step: 2\n",
      "Action: [-0.07589445 -0.00952268 -0.14967984  0.36333537  0.7685577   0.45650908\n",
      " -0.4897668  -0.06565165  0.13162467 -0.09120943  0.5308131   0.6147827 ]\n",
      "Observations: [-0.03948344 -0.03942708 -0.88756329  0.10416517 -0.09843008 -0.97133595\n",
      "  0.03793851 -0.06321084 -0.97719961 -0.08852407 -0.02009471 -0.94415182\n",
      "  0.02642515  0.01998134]\n",
      "Reward: -0.5338438293796889\n",
      "Info {'reward:': -0.5338438293796889, 'angle_reward': 0.40009328476475203, 'z_position_reward': 0.06606288585555908}\n",
      "Step: 3\n",
      "Action: [-0.83651036  0.8041252  -0.84824896  0.6873332   0.31878778 -0.4120299\n",
      "  0.8194587   0.7040196  -0.19116618  0.4602566  -0.9595161   0.12785031]\n",
      "Observations: [-0.04707288 -0.04037935 -0.90253128  0.14049871 -0.02157431 -0.92568504\n",
      " -0.01103817 -0.069776   -0.96403714 -0.09764501  0.0329866  -0.88267356\n",
      "  0.02903741  0.00876879]\n",
      "Reward: -0.4712504462428223\n",
      "Info {'reward:': -0.4712504462428223, 'angle_reward': 0.4561560365425983, 'z_position_reward': 0.0725935172145794}\n",
      "Step: 4\n",
      "Action: [-0.69748974  0.04693041 -0.7067165  -0.8625879   0.5270446   0.13280247\n",
      " -0.36837843 -0.32898527  0.13767129  0.70227736 -0.0184883  -0.97712696]\n",
      "Observations: [-1.30723917e-01  4.00331679e-02 -9.87356171e-01  2.09232034e-01\n",
      "  1.03044692e-02 -9.66888031e-01  7.09077060e-02  6.25958294e-04\n",
      " -9.83153759e-01 -5.16193531e-02 -6.29650116e-02 -8.69888525e-01\n",
      "  3.01473456e-02  3.81778658e-03]\n",
      "Reward: -0.44372056899681045\n",
      "Info {'reward:': -0.44372056899681045, 'angle_reward': 0.48091106708032033, 'z_position_reward': 0.07536836392286922}\n",
      "Step: 5\n",
      "Action: [ 0.23147075  0.30758965  0.2916767  -0.50057703  0.52651924  0.60575014\n",
      " -0.7766474  -0.47765     0.44868892 -0.01668153  0.25728634 -0.8012983 ]\n",
      "Observations: [-0.20047289  0.04472621 -1.          0.12297324  0.06300893 -0.95360778\n",
      "  0.03406986 -0.03227257 -0.96938663  0.01860838 -0.06481384 -0.96760122\n",
      "  0.03062813  0.00166986]\n",
      "Reward: -0.4317789707942863\n",
      "Info {'reward:': -0.4317789707942863, 'angle_reward': 0.491650706154915, 'z_position_reward': 0.07657032305079864}\n",
      "Step: 6\n",
      "Action: [ 0.22680235  0.6684538  -0.70207167 -0.40471748 -0.6967154  -0.0587901\n",
      "  0.18523474 -0.33876252 -0.6181747  -0.9375841  -0.48190054 -0.2633732 ]\n",
      "Observations: [-1.77325816e-01  7.54851739e-02 -9.70832330e-01  7.29155377e-02\n",
      "  1.15660853e-01 -8.93032770e-01 -4.35948759e-02 -8.00375678e-02\n",
      " -9.24517737e-01  1.69402303e-02 -3.90852077e-02 -1.00000000e+00\n",
      "  3.08354143e-02  7.34542530e-04]\n",
      "Reward: -0.42658417684544747\n",
      "Info {'reward:': -0.42658417684544747, 'angle_reward': 0.49632728734901166, 'z_position_reward': 0.07708853580554093}\n",
      "Step: 7\n",
      "Action: [-0.5553978  -0.226167    0.690979   -0.7238663  -0.8300494   0.7479992\n",
      " -0.8295191  -0.5700784   0.6546881   0.14982349  0.56447256  0.883998  ]\n",
      "Observations: [-1.54645582e-01  1.42330555e-01 -1.00000000e+00  3.24437901e-02\n",
      "  4.59893111e-02 -8.98911780e-01 -2.50714019e-02 -1.13913820e-01\n",
      " -9.86335205e-01 -7.68181799e-02 -8.72752620e-02 -1.00000000e+00\n",
      "  3.09239685e-02  3.23828206e-04]\n",
      "Reward: -0.42430921986789216\n",
      "Info {'reward:': -0.42430921986789216, 'angle_reward': 0.4983808589680154, 'z_position_reward': 0.07730992116409235}\n",
      "Step: 8\n",
      "Action: [ 0.40353304 -0.05173822 -0.5511148  -0.8783544  -0.3163125  -0.9183219\n",
      " -0.49788356  0.6550971   0.92717403  0.2125232  -0.17089923  0.84923476]\n",
      "Observations: [-2.10185362e-01  1.19713856e-01 -9.30902100e-01 -3.99428383e-02\n",
      " -3.70156284e-02 -8.24111861e-01 -1.08023311e-01 -1.70921657e-01\n",
      " -9.20866393e-01 -6.18358312e-02 -3.08280064e-02 -9.11600202e-01\n",
      "  3.09614984e-02  1.38246869e-04]\n",
      "Reward: -0.4232874883732569\n",
      "Info {'reward:': -0.4232874883732569, 'angle_reward': 0.4993087656572898, 'z_position_reward': 0.07740374596945326}\n",
      "Step: 9\n",
      "Action: [-0.699008    0.62461466 -0.4090886   0.13811535 -0.7915644   0.04111515\n",
      "  0.3535973  -0.5187992  -0.5419436  -0.37758824  0.8286341  -0.30319124]\n",
      "Observations: [-1.69832058e-01  1.14540033e-01 -9.86013579e-01 -1.27778275e-01\n",
      " -6.86468776e-02 -9.15944051e-01 -1.57811667e-01 -1.05411945e-01\n",
      " -8.28148989e-01 -4.05835105e-02 -4.79179291e-02 -8.26676726e-01\n",
      "  3.09780415e-02  6.20817615e-05]\n",
      "Reward: -0.4228653049632274\n",
      "Info {'reward:': -0.4228653049632274, 'angle_reward': 0.49968959119272655, 'z_position_reward': 0.07744510384404601}\n",
      "Step: 10\n",
      "Action: [ 0.7276059  -0.20784195  0.18889447 -0.519872   -0.49556604  0.80431926\n",
      "  0.77386487 -0.09274338 -0.7004104  -0.41266716 -0.70333457  0.32830554]\n",
      "Observations: [-0.23973286  0.1770015  -1.         -0.11396674 -0.14780332 -0.91183254\n",
      " -0.12245194 -0.15729186 -0.88234335 -0.07834233  0.03494548 -0.85699585\n",
      "  0.03148047  0.00392772]\n",
      "Reward: -0.44093741136836795\n",
      "Info {'reward:': -0.44093741136836795, 'angle_reward': 0.4803614079267505, 'z_position_reward': 0.07870118070488152}\n",
      "Step: 11\n",
      "Action: [ 0.74505097  0.7805141   0.5767532  -0.5933591  -0.7014158   0.9061225\n",
      " -0.582104   -0.14580406  0.8916421   0.02602728  0.0203068  -0.04538279]\n",
      "Observations: [-0.16697227  0.1562173  -0.98111055 -0.16595394 -0.19735992 -0.83140061\n",
      " -0.04506545 -0.1665662  -0.95238439 -0.11960905 -0.03538798 -0.8241653\n",
      "  0.0317477   0.0040064 ]\n",
      "Reward: -0.4406627382217958\n",
      "Info {'reward:': -0.4406627382217958, 'angle_reward': 0.4799680120867861, 'z_position_reward': 0.07936924969141812}\n",
      "Step: 12\n",
      "Action: [-0.18970947  0.8415789   0.03048244 -0.26908177  0.22106586 -0.5348248\n",
      " -0.8997465  -0.8236296  -0.6627288   0.74345285  0.618211    0.6629172 ]\n",
      "Observations: [-0.09246717  0.23426872 -0.92343523 -0.22528985 -0.2675015  -0.74078836\n",
      " -0.10327585 -0.18114661 -0.86322018 -0.11700632 -0.0333573  -0.82870358\n",
      "  0.03308259  0.00870005]\n",
      "Reward: -0.4607937678145694\n",
      "Info {'reward:': -0.4607937678145694, 'angle_reward': 0.45649975307408225, 'z_position_reward': 0.08270647911134837}\n",
      "Step: 13\n",
      "Action: [-0.55278534  0.8243402   0.0744245  -0.952275   -0.09476617 -0.41832274\n",
      "  0.44830388  0.36837572 -0.44719353 -0.689904    0.61129385  0.9034625 ]\n",
      "Observations: [-0.11143812  0.31842661 -0.92038699 -0.25219803 -0.24539491 -0.79427084\n",
      " -0.1932505  -0.26350957 -0.92949306 -0.04266104  0.0284638  -0.76241186\n",
      "  0.03507651  0.01653044]\n",
      "Reward: -0.4949609049616366\n",
      "Info {'reward:': -0.4949609049616366, 'angle_reward': 0.41734781204391036, 'z_position_reward': 0.087691282994453}\n",
      "Step: 14\n",
      "Action: [-0.21129204  0.92382175 -0.33256316 -0.8457544  -0.965955   -0.04414082\n",
      "  0.17251822 -0.67506015  0.8030799  -0.6798179  -0.24093308  0.29972845]\n",
      "Observations: [-0.16671665  0.40086063 -0.91294454 -0.34742553 -0.25487153 -0.83610311\n",
      " -0.14842011 -0.226672   -0.97421242 -0.11165143  0.08959318 -0.6720656\n",
      "  0.03712119  0.02486494]\n",
      "Reward: -0.5315217141951656\n",
      "Info {'reward:': -0.5315217141951656, 'angle_reward': 0.37567530979028513, 'z_position_reward': 0.09280297601454929}\n",
      "Step: 15\n",
      "Action: [-0.79432887  0.7325547  -0.2536355  -0.51485914  0.12165693  0.68247855\n",
      " -0.22628267  0.60089415  0.2889198  -0.2775328  -0.18306763 -0.6608019 ]\n",
      "Observations: [-0.18784586  0.4932428  -0.94620086 -0.43200097 -0.35146703 -0.84051719\n",
      " -0.13116829 -0.29417801 -0.89390443 -0.17963323  0.06549988 -0.64209276\n",
      "  0.03919351  0.03346177]\n",
      "Reward: -0.5693250725430982\n",
      "Info {'reward:': -0.5693250725430982, 'angle_reward': 0.33269114040890596, 'z_position_reward': 0.09798378704799587}\n",
      "Step: 16\n",
      "Action: [-0.02404693 -0.2006661  -0.09508384  0.4033767   0.02289278  0.60814476\n",
      "  0.04450003  0.7128606  -0.4560205  -0.44094843  0.3412427   0.32850373]\n",
      "Observations: [-0.26727874  0.56649827 -0.97156441 -0.48348688 -0.33930134 -0.77226934\n",
      " -0.15379656 -0.2340886  -0.86501244 -0.2073865   0.04719311 -0.70817295\n",
      "  0.04093005  0.04079475]\n",
      "Reward: -0.6016486576766682\n",
      "Info {'reward:': -0.6016486576766682, 'angle_reward': 0.29602622914943455, 'z_position_reward': 0.1023251131738972}\n",
      "Step: 17\n",
      "Action: [ 0.5670536  -0.31932235 -0.10684137 -0.64150596  0.46445134  0.73383355\n",
      " -0.25000897  0.7466683  -0.56312174  0.1680156  -0.12030945 -0.9684394 ]\n",
      "Observations: [-0.26968344  0.54643166 -0.98107279 -0.44314921 -0.33701206 -0.71145486\n",
      " -0.14934655 -0.16280254 -0.91061449 -0.25148135  0.08131738 -0.67532257\n",
      "  0.04120017  0.04188458]\n",
      "Reward: -0.6064224913601773\n",
      "Info {'reward:': -0.6064224913601773, 'angle_reward': 0.2905770887373589, 'z_position_reward': 0.10300041990246378}\n",
      "Step: 18\n",
      "Action: [-0.361961    0.73655915  0.57570595  0.48191425 -0.99191093  0.14689186\n",
      "  0.1147792   0.89245635 -0.03766746  0.9777479  -0.87233293 -0.43836   ]\n",
      "Observations: [-0.21297808  0.51449943 -0.99175693 -0.50729981 -0.29056693 -0.63807151\n",
      " -0.17434745 -0.08813571 -0.96692667 -0.23467979  0.06928644 -0.77216651\n",
      "  0.04093678  0.04081868]\n",
      "Reward: -0.6017514508522657\n",
      "Info {'reward:': -0.6017514508522657, 'angle_reward': 0.29590659257614627, 'z_position_reward': 0.10234195657158801}\n",
      "Step: 19\n",
      "Action: [-0.70235676  0.7927249  -0.4514177   0.27424505 -0.5008657  -0.88137555\n",
      "  0.6740958   0.04787289  0.26874667 -0.5982493  -0.09876998 -0.87432796]\n",
      "Observations: [-0.24917418  0.58815534 -0.93418633 -0.45910838 -0.38975802 -0.62338232\n",
      " -0.16286953  0.00110992 -0.97069341 -0.136905   -0.01794685 -0.81600251\n",
      "  0.04288226  0.04129564]\n",
      "Reward: -0.5992725485210975\n",
      "Info {'reward:': -0.5992725485210975, 'angle_reward': 0.2935217897074165, 'z_position_reward': 0.1072056617714861}\n",
      "Step: 20\n",
      "Action: [ 0.15410353 -0.01670129  0.75477093 -0.6100658  -0.60358304 -0.11665781\n",
      " -0.97622806 -0.9742448   0.92042994  0.27388385  0.52778476  0.6489493 ]\n",
      "Observations: [-0.31940985  0.66742783 -0.9793281  -0.43168388 -0.43984459 -0.71151988\n",
      " -0.09545995  0.00589721 -0.94381875 -0.19672993 -0.02782385 -0.90343531\n",
      "  0.0477922   0.04294733]\n",
      "Reward: -0.5952561481888226\n",
      "Info {'reward:': -0.5952561481888226, 'angle_reward': 0.28526334382823343, 'z_position_reward': 0.11948050798294396}\n",
      "Step: 21\n",
      "Action: [ 0.11963183  0.42737055 -0.1608049   0.7022123  -0.73663473 -0.07016422\n",
      "  0.10599075  0.4830147   0.56203514 -0.14555895 -0.08325671  0.13076462]\n",
      "Observations: [-0.3039995   0.6657577  -0.90385101 -0.49269046 -0.50020289 -0.72318566\n",
      " -0.19308276 -0.09152726 -0.85177575 -0.16934154  0.02495462 -0.83854038\n",
      "  0.04547641  0.06009969]\n",
      "Reward: -0.6868074141538638\n",
      "Info {'reward:': -0.6868074141538638, 'angle_reward': 0.1995015592753276, 'z_position_reward': 0.11369102657080854}\n",
      "Step: 22\n",
      "Action: [ 0.99488556 -0.84563386 -0.11316744 -0.31762707  0.32800806  0.6744803\n",
      "  0.30645776  0.52667814  0.2064288   0.47229132 -0.9443637   0.29502532]\n",
      "Observations: [-0.29203632  0.70849476 -0.9199315  -0.42246923 -0.57386637 -0.73020208\n",
      " -0.18248368 -0.04322579 -0.79557224 -0.18389744  0.01662895 -0.82546392\n",
      "  0.0470347   0.05987408]\n",
      "Reward: -0.681783637199394\n",
      "Info {'reward:': -0.681783637199394, 'angle_reward': 0.20062960149110817, 'z_position_reward': 0.11758676130949774}\n",
      "Step: 23\n",
      "Action: [ 0.48456508 -0.49340588 -0.17000097  0.53242356  0.26279196  0.5419904\n",
      " -0.68307155  0.7625134   0.5837332   0.23336005 -0.37957707  0.02067009]\n",
      "Observations: [-0.19254776  0.62393137 -0.93124824 -0.45423194 -0.54106556 -0.66275405\n",
      " -0.15183791  0.00944202 -0.77492936 -0.13666831 -0.07780742 -0.79596138\n",
      "  0.04767015  0.06984681]\n",
      "Reward: -0.7300586818932879\n",
      "Info {'reward:': -0.7300586818932879, 'angle_reward': 0.15076594075776875, 'z_position_reward': 0.11917537734894329}\n",
      "Step: 24\n",
      "Action: [ 0.3091129  -0.18435992  0.6297206  -0.04762208  0.9066317  -0.4477115\n",
      "  0.80687475  0.97743285 -0.06223361  0.7743817   0.42388543  0.7632135 ]\n",
      "Observations: [-0.14409125  0.57459078 -0.94824834 -0.40098958 -0.51478636 -0.60855501\n",
      " -0.22014506  0.08569336 -0.71655604 -0.1133323  -0.11576513 -0.79389438\n",
      "  0.05060298  0.07561889]\n",
      "Reward: -0.7515869901550623\n",
      "Info {'reward:': -0.7515869901550623, 'angle_reward': 0.121905556433482, 'z_position_reward': 0.12650745341145575}\n",
      "Step: 25\n",
      "Action: [ 0.29707387  0.31179821  0.43297333 -0.30681184 -0.4392281   0.61468697\n",
      " -0.711849   -0.86619484  0.9333997  -0.04258114  0.22882028  0.8412743 ]\n",
      "Observations: [-0.11317996  0.55615479 -0.88527628 -0.40575179 -0.42412319 -0.65332616\n",
      " -0.13945759  0.18343665 -0.7227794  -0.03589413 -0.07337658 -0.71757302\n",
      "  0.05418348  0.07594849]\n",
      "Reward: -0.7442837322390168\n",
      "Info {'reward:': -0.7442837322390168, 'angle_reward': 0.12025756586878641, 'z_position_reward': 0.13545870189219678}\n",
      "Step: 26\n",
      "Action: [ 0.9265197  -0.68996423  0.09548791 -0.45112285  0.26550138 -0.32614857\n",
      " -0.29323092 -0.993979    0.6741944  -0.20922498  0.62177783 -0.92809474]\n",
      "Observations: [-0.08347257  0.58733461 -0.84197895 -0.43643297 -0.468046   -0.59185746\n",
      " -0.21064248  0.09681716 -0.62943943 -0.04015224 -0.05049455 -0.63344559\n",
      "  0.05443752  0.0802426 ]\n",
      "Reward: -0.7651192038467622\n",
      "Info {'reward:': -0.7651192038467622, 'angle_reward': 0.09878699187126978, 'z_position_reward': 0.136093804281968}\n",
      "Step: 27\n",
      "Action: [ 0.7135986  -0.10160701  0.9582696  -0.7125102   0.18139534 -0.5028757\n",
      "  0.06420349 -0.73854595 -0.2889296  -0.79563665 -0.9520097  -0.5093433 ]\n",
      "Observations: [ 0.0091794   0.51833819 -0.83243015 -0.48154526 -0.44149586 -0.62447232\n",
      " -0.23996558 -0.00258074 -0.56201999 -0.06107474  0.01168323 -0.72625507\n",
      "  0.05058767  0.06492551]\n",
      "Reward: -0.6981583970199442\n",
      "Info {'reward:': -0.6981583970199442, 'angle_reward': 0.17537242594897778, 'z_position_reward': 0.12646917703107802}\n",
      "Step: 28\n",
      "Action: [ 0.5803451  -0.13606809 -0.36601463 -0.49463218 -0.64724386  0.7161579\n",
      " -0.2314117  -0.67954993  0.44519347  0.06850374  0.787741   -0.24569976]\n",
      "Observations: [ 0.08053926  0.50817749 -0.73660319 -0.55279628 -0.42335633 -0.67475989\n",
      " -0.23354523 -0.07643533 -0.59091295 -0.14063841 -0.08351774 -0.7771894\n",
      "  0.04263578  0.05086977]\n",
      "Reward: -0.6477593985206245\n",
      "Info {'reward:': -0.6477593985206245, 'angle_reward': 0.24565114265658708, 'z_position_reward': 0.10658945882278847}\n",
      "Step: 29\n",
      "Action: [ 0.52055603 -0.20582516 -0.93199915  0.44694942  0.27884525 -0.41492322\n",
      "  0.11683054 -0.9366869  -0.8791442  -0.98350054 -0.48741496 -0.16389278]\n",
      "Observations: [ 0.13857377  0.49457068 -0.77320466 -0.6022595  -0.48808072 -0.6031441\n",
      " -0.2566864  -0.14439033 -0.54639361 -0.13378803 -0.00474364 -0.80175938\n",
      "  0.04070724  0.04158711]\n",
      "Reward: -0.6061674195985992\n",
      "Info {'reward:': -0.6061674195985992, 'angle_reward': 0.2920644699874682, 'z_position_reward': 0.10176811041393258}\n",
      "Step: 30\n",
      "Action: [ 0.01698789  0.4668725  -0.45116737 -0.42601043  0.6819868  -0.67877567\n",
      " -0.38722116 -0.2944663   0.20742728 -0.45958024  0.6998975   0.00882755]\n",
      "Observations: [ 0.19062937  0.47398816 -0.86640457 -0.55756456 -0.46019619 -0.64463642\n",
      " -0.24500334 -0.23805901 -0.63430803 -0.23213809 -0.05348513 -0.81814865\n",
      "  0.04021744  0.03762344]\n",
      "Reward: -0.587573593973802\n",
      "Info {'reward:': -0.587573593973802, 'angle_reward': 0.31188279689542203, 'z_position_reward': 0.10054360913077592}\n",
      "Step: 31\n",
      "Action: [-0.6472364   0.19642772 -0.2466827   0.6497805  -0.3989973  -0.5862237\n",
      " -0.86484015  0.31240404  0.2409603  -0.19267228 -0.7150983   0.7680902 ]\n",
      "Observations: [ 0.19232816  0.52067541 -0.91152131 -0.6001656  -0.39199751 -0.71251398\n",
      " -0.28372546 -0.26750564 -0.6135653  -0.27809611  0.01650462 -0.8172659\n",
      "  0.04072641  0.0398983 ]\n",
      "Reward: -0.5976754731027918\n",
      "Info {'reward:': -0.5976754731027918, 'angle_reward': 0.3005084920608829, 'z_position_reward': 0.10181603483632529}\n",
      "Step: 32\n",
      "Action: [ 0.82376236 -0.48728073  0.73651165  0.42718133  0.71467835  0.19193672\n",
      "  0.8405846  -0.76324826  0.376281    0.314411    0.77501076 -0.99580735]\n",
      "Observations: [ 0.12760452  0.54031819 -0.93618958 -0.53518755 -0.43189724 -0.77113636\n",
      " -0.37020947 -0.23626524 -0.58946927 -0.29736334 -0.05500521 -0.74045688\n",
      "  0.04127395  0.04235318]\n",
      "Reward: -0.608581013686998\n",
      "Info {'reward:': -0.608581013686998, 'angle_reward': 0.2882341214539995, 'z_position_reward': 0.10318486485900247}\n",
      "Step: 33\n",
      "Action: [-0.27841341 -0.8124367   0.98511106 -0.5867347  -0.35159525  0.5033897\n",
      " -0.6980872  -0.9556601  -0.01489232  0.5557965  -0.26912448 -0.4807158 ]\n",
      "Observations: [ 0.20998075  0.49159011 -0.86253842 -0.49246942 -0.36042941 -0.75194268\n",
      " -0.28615101 -0.31259006 -0.55184117 -0.26592224  0.02249586 -0.84003761\n",
      "  0.04056108  0.03917868]\n",
      "Reward: -0.5944906850640401\n",
      "Info {'reward:': -0.5944906850640401, 'angle_reward': 0.3041066182915524, 'z_position_reward': 0.10140269664440754}\n",
      "Step: 34\n",
      "Action: [-0.93639106  0.87935376  0.72652745 -0.9019005   0.4260148  -0.75471425\n",
      "  0.06828794 -0.24835338 -0.81177515 -0.49175826  0.5635966   0.16549867]\n",
      "Observations: [ 0.18213941  0.41034644 -0.76402731 -0.55114289 -0.39558893 -0.70160371\n",
      " -0.35595974 -0.40815607 -0.5533304  -0.21034259 -0.00441658 -0.8881092\n",
      "  0.03933417  0.03395712]\n",
      "Reward: -0.5714501769942673\n",
      "Info {'reward:': -0.5714501769942673, 'angle_reward': 0.33021438651775037, 'z_position_reward': 0.09833543648798243}\n",
      "Step: 35\n",
      "Action: [ 0.67489153  0.03696429 -0.2856857  -0.4891444  -0.31037667 -0.29798302\n",
      "  0.98224396 -0.52508587 -0.684089    0.5192006  -0.54279715  0.9563389 ]\n",
      "Observations: [ 0.08850031  0.49828182 -0.69137456 -0.64133293 -0.35298745 -0.77707514\n",
      " -0.34913094 -0.43299141 -0.63450792 -0.25951842  0.05194308 -0.87155933\n",
      "  0.0401423   0.03729852]\n",
      "Reward: -0.5861368637201672\n",
      "Info {'reward:': -0.5861368637201672, 'angle_reward': 0.3135073984464948, 'z_position_reward': 0.10035573783333801}\n",
      "Step: 36\n",
      "Action: [ 0.3155102  -0.90586656  0.21426179 -0.7848652  -0.7078975   0.8360567\n",
      "  0.30842638  0.17988944  0.27920544  0.5058982   0.374452   -0.25313258]\n",
      "Observations: [ 0.15598946  0.50197825 -0.71994313 -0.69024738 -0.38402512 -0.80687344\n",
      " -0.25090655 -0.4855     -0.70291682 -0.20759835 -0.00233664 -0.77592544\n",
      "  0.04035904  0.03822726]\n",
      "Reward: -0.5902386663380822\n",
      "Info {'reward:': -0.5902386663380822, 'angle_reward': 0.30886372232055676, 'z_position_reward': 0.10089761134136105}\n",
      "Step: 37\n",
      "Action: [-0.8066534  -0.65787065  0.3375487   0.774052   -0.8243012  -0.4018873\n",
      " -0.4332483  -0.22668633 -0.7612234   0.27027488 -0.41561478 -0.5551693 ]\n",
      "Observations: [ 0.18754048  0.41139159 -0.69851695 -0.7687339  -0.45481487 -0.72326777\n",
      " -0.22006391 -0.46751105 -0.67499627 -0.15700854  0.03510856 -0.8012387\n",
      "  0.03924093  0.03360975]\n",
      "Reward: -0.5699464088580593\n",
      "Info {'reward:': -0.5699464088580593, 'angle_reward': 0.3319512544672065, 'z_position_reward': 0.09810233667473423}\n",
      "Step: 38\n",
      "Action: [-0.6771605  -0.73783714  0.43313557  0.3357354   0.6500505  -0.9906143\n",
      " -0.8639455  -0.0406041  -0.3670143   0.07951599  0.5381204  -0.8537689 ]\n",
      "Observations: [ 0.10687514  0.34560453 -0.66476208 -0.69132869 -0.53724499 -0.7634565\n",
      " -0.26338874 -0.49017969 -0.75111861 -0.12998105 -0.00645292 -0.85675563\n",
      "  0.03778424  0.02759808]\n",
      "Reward: -0.5435298010995218\n",
      "Info {'reward:': -0.5435298010995218, 'angle_reward': 0.3620096062523842, 'z_position_reward': 0.09446059264809403}\n",
      "Step: 39\n",
      "Action: [ 0.5994712   0.107085   -0.73263586  0.8926183   0.5882034   0.40372035\n",
      "  0.8953473   0.35027054  0.17395505  0.54340076  0.69976157 -0.44745636]\n",
      "Observations: [ 0.03915909  0.27182081 -0.62144853 -0.65775515 -0.47223993 -0.86251793\n",
      " -0.34978329 -0.4942401  -0.78782004 -0.12202945  0.04735912 -0.94213251\n",
      "  0.03610428  0.02068435]\n",
      "Reward: -0.5131610319211273\n",
      "Info {'reward:': -0.5131610319211273, 'angle_reward': 0.39657827042787264, 'z_position_reward': 0.09026069765100014}\n",
      "Step: 40\n",
      "Action: [-0.6232745  -0.12786256 -0.84478605  0.5265771   0.4456128   0.49839768\n",
      " -0.5796767  -0.36099884  0.86733055  0.30288553 -0.98387325  0.13136674]\n",
      "Observations: [ 0.09910621  0.28252931 -0.69471211 -0.56849332 -0.4134196  -0.82214589\n",
      " -0.26024856 -0.45921304 -0.77042453 -0.06768937  0.11733528 -0.98687815\n",
      "  0.03559807  0.01860031]\n",
      "Reward: -0.5040063863231214\n",
      "Info {'reward:': -0.5040063863231214, 'angle_reward': 0.40699843610026915, 'z_position_reward': 0.08899517757660935}\n",
      "Step: 41\n",
      "Action: [-0.97551787  0.21743165  0.76640064 -0.8634105   0.73892695  0.6326328\n",
      " -0.54956853 -0.5127918  -0.911207    0.31389403 -0.5581549   0.6920817 ]\n",
      "Observations: [ 0.03677876  0.26974306 -0.77919072 -0.51583561 -0.36885832 -0.77230613\n",
      " -0.31821623 -0.49531293 -0.68369148 -0.03740082  0.01894795 -0.97374148\n",
      "  0.03521512  0.01707152]\n",
      "Reward: -0.49731979198716936\n",
      "Info {'reward:': -0.49731979198716936, 'angle_reward': 0.41464241184918177, 'z_position_reward': 0.08803779616364887}\n",
      "Step: 42\n",
      "Action: [ 0.02169131  0.8966856   0.77136064 -0.6125238  -0.7055445   0.9141733\n",
      "  0.1648817   0.4117859  -0.5543553   0.9384251   0.15070924  0.5764391 ]\n",
      "Observations: [-0.06077302  0.29148622 -0.70255065 -0.60217666 -0.29496562 -0.70904285\n",
      " -0.37317308 -0.54659211 -0.77481218 -0.00601141 -0.03686753 -0.90453331\n",
      "  0.03528857  0.01726578]\n",
      "Reward: -0.49810746762523506\n",
      "Info {'reward:': -0.49810746762523506, 'angle_reward': 0.4136711072247994, 'z_position_reward': 0.08822142514996562}\n",
      "Step: 43\n",
      "Action: [ 0.389139   -0.49638763 -0.8891134   0.53546906 -0.9393433   0.63438356\n",
      " -0.3057964  -0.8041872  -0.12653443 -0.01986952 -0.5581504  -0.61941147]\n",
      "Observations: [-0.05860389  0.38115478 -0.62541459 -0.66342904 -0.36552007 -0.61762552\n",
      " -0.35668491 -0.50541352 -0.83024771  0.0878311  -0.02179661 -0.8468894\n",
      "  0.0368216   0.0236396 ]\n",
      "Reward: -0.5261440055916711\n",
      "Info {'reward:': -0.5261440055916711, 'angle_reward': 0.3818019987095373, 'z_position_reward': 0.09205399569879158}\n",
      "Step: 44\n",
      "Action: [ 0.64429104  0.71581227 -0.24129058  0.5205521   0.13207836 -0.8564392\n",
      "  0.4390991   0.2203725  -0.13443366 -0.6729874   0.5919304  -0.3120046 ]\n",
      "Observations: [-0.01968999  0.33151602 -0.71432593 -0.60988213 -0.4594544  -0.55418716\n",
      " -0.38726455 -0.58583224 -0.84290116  0.08584414 -0.07761165 -0.90883055\n",
      "  0.03670111  0.02317085]\n",
      "Reward: -0.5241014761098015\n",
      "Info {'reward:': -0.5241014761098015, 'angle_reward': 0.3841457407422588, 'z_position_reward': 0.09175278314793972}\n",
      "Step: 45\n",
      "Action: [-0.33972937 -0.2022055  -0.10620297  0.6140046  -0.07353277  0.81812364\n",
      " -0.09079736 -0.97019106  0.14242466 -0.59223974 -0.8654199   0.28591874]\n",
      "Observations: [ 0.04473911  0.40309725 -0.73845499 -0.55782692 -0.44624656 -0.63983108\n",
      " -0.34335464 -0.56379499 -0.85634452  0.0185454  -0.01841861 -0.94003101\n",
      "  0.0376774   0.0271184 ]\n",
      "Reward: -0.5413984919306581\n",
      "Info {'reward:': -0.5413984919306581, 'angle_reward': 0.3644080091970515, 'z_position_reward': 0.09419349887229034}\n",
      "Step: 46\n",
      "Action: [ 0.79139125  0.21705204 -0.5548724   0.51983005 -0.7226804   0.7003063\n",
      "  0.45323864 -0.7799631   0.4321867   0.7115294  -0.5005218  -0.36893225]\n",
      "Observations: [ 0.01076617  0.3828767  -0.74907529 -0.49642646 -0.45359984 -0.55801871\n",
      " -0.35243438 -0.66081409 -0.84210206 -0.04067857 -0.1049606  -0.91143913\n",
      "  0.03772869  0.02741187]\n",
      "Reward: -0.5427376164091826\n",
      "Info {'reward:': -0.5427376164091826, 'angle_reward': 0.3629406706898584, 'z_position_reward': 0.09432171290095903}\n",
      "Step: 47\n",
      "Action: [ 0.5713366  -0.2216134  -0.48333985  0.65679586 -0.49106374 -0.8916052\n",
      " -0.36597186  0.10449181  0.18528008 -0.2075259  -0.6586085  -0.2293947 ]\n",
      "Observations: [ 0.0899053   0.4045819  -0.80456253 -0.44444346 -0.52586788 -0.48798808\n",
      " -0.30711051 -0.7388104  -0.79888339  0.03047437 -0.15501278 -0.94833236\n",
      "  0.03783322  0.02772041]\n",
      "Reward: -0.5440189825673065\n",
      "Info {'reward:': -0.5440189825673065, 'angle_reward': 0.3613979695008741, 'z_position_reward': 0.09458304793181943}\n",
      "Step: 48\n",
      "Action: [-0.38325432 -0.1379877  -0.6768509   0.30917895  0.83783     0.9932649\n",
      " -0.32392153 -0.7621185  -0.8076267   0.3273613   0.6525566   0.6331539 ]\n",
      "Observations: [ 0.14703896  0.38242056 -0.85289651 -0.37876387 -0.57497425 -0.5771486\n",
      " -0.3437077  -0.72836122 -0.78035538  0.00972178 -0.22087363 -0.97127183\n",
      "  0.03766469  0.0271089 ]\n",
      "Reward: -0.5413827955416645\n",
      "Info {'reward:': -0.5413827955416645, 'angle_reward': 0.3644554832862972, 'z_position_reward': 0.09416172117203836}\n",
      "Step: 49\n",
      "Action: [-0.42775127  0.16934673 -0.25211775  0.06736408  0.2135309  -0.36784124\n",
      " -0.8267314   0.33855847  0.01805864 -0.23893811  0.07513612  0.97211564]\n",
      "Observations: [ 0.10871353  0.36862179 -0.9205816  -0.34784598 -0.49119125 -0.47782211\n",
      " -0.37609985 -0.80457307 -0.86111805  0.04245791 -0.15561797 -0.90795643\n",
      "  0.03753575  0.02659771]\n",
      "Reward: -0.5391491817051268\n",
      "Info {'reward:': -0.5391491817051268, 'angle_reward': 0.36701144292505233, 'z_position_reward': 0.09383937536982079}\n",
      "Step: 50\n",
      "Action: [-0.8683283   0.8822173  -0.8590412  -0.05960449 -0.5422685  -0.92803204\n",
      "  0.9205563  -0.8303011   0.20346253 -0.4649966  -0.71395874 -0.25292736]\n",
      "Observations: [ 0.0659384   0.38555646 -0.94579338 -0.34110957 -0.46983816 -0.51460624\n",
      " -0.45877299 -0.77071723 -0.85931219  0.0185641  -0.14810436 -0.81074487\n",
      "  0.0376665   0.02714277]\n",
      "Reward: -0.5415475694180676\n",
      "Info {'reward:': -0.5415475694180676, 'angle_reward': 0.3642861685685897, 'z_position_reward': 0.09416626201334272}\n",
      "Step: 51\n",
      "Action: [-0.36990368 -0.27490664  0.91583717 -0.51840764  0.8174503   0.46847698\n",
      "  0.99411595  0.9698055  -0.26595768  0.7555999  -0.6927083   0.73224646]\n",
      "Observations: [-0.02089442  0.47377819 -1.         -0.34707002 -0.52406501 -0.60740944\n",
      " -0.36671736 -0.85374734 -0.83896593 -0.02793556 -0.21950023 -0.83603761\n",
      "  0.03903489  0.03286216]\n",
      "Reward: -0.5667235703537086\n",
      "Info {'reward:': -0.5667235703537086, 'angle_reward': 0.33568919304165173, 'z_position_reward': 0.0975872366046397}\n",
      "Step: 52\n",
      "Action: [ 0.47511682  0.48286957  0.609456    0.06830467 -0.26338536  0.59707105\n",
      "  0.6706716  -0.2395173   0.38527337  0.01606528  0.31782252 -0.17763333]\n",
      "Observations: [-0.05788479  0.44628753 -0.90841628 -0.39891078 -0.44231998 -0.56056174\n",
      " -0.26730577 -0.75676679 -0.8655617   0.04762443 -0.28877107 -0.76281296\n",
      "  0.03912135  0.03304871]\n",
      "Reward: -0.5674402036476462\n",
      "Info {'reward:': -0.5674402036476462, 'angle_reward': 0.3347564252203609, 'z_position_reward': 0.09780337113199278}\n",
      "Step: 53\n",
      "Action: [-0.6983736  -0.09830655  0.38358036  0.16038907 -0.3583925  -0.1868062\n",
      " -0.1999695   0.94754916 -0.03531699 -0.6196555  -0.05644168  0.28097644]\n",
      "Observations: [-0.01037311  0.49457448 -0.84747068 -0.39208031 -0.46865852 -0.50085464\n",
      " -0.20023861 -0.78071852 -0.82703437  0.04923096 -0.25698881 -0.78057629\n",
      "  0.0398224   0.03607578]\n",
      "Reward: -0.5808228780860645\n",
      "Info {'reward:': -0.5808228780860645, 'angle_reward': 0.3196211182252742, 'z_position_reward': 0.09955600368866135}\n",
      "Step: 54\n",
      "Action: [ 0.3763983  -0.44038063  0.19809975 -0.7337735   0.54495573 -0.90121126\n",
      "  0.5090294   0.60158974 -0.8410001  -0.6029781  -0.59514403  0.0630684 ]\n",
      "Observations: [-0.08021047  0.48474383 -0.80911265 -0.37604141 -0.50449777 -0.51953526\n",
      " -0.22023556 -0.6859636  -0.83056606 -0.01273459 -0.26263298 -0.75247865\n",
      "  0.03977502  0.03580321]\n",
      "Reward: -0.5795784884604877\n",
      "Info {'reward:': -0.5795784884604877, 'angle_reward': 0.32098396285275316, 'z_position_reward': 0.09943754868675919}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminated:\n\u001b[0;32m     17\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m---> 18\u001b[0m     obs, rew, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Print the tobservations, reward\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep:\u001b[39m\u001b[38;5;124m\"\u001b[39m, i)\n",
      "File \u001b[1;32mc:\\Users\\dorva\\Documents\\GitHub\\dorvan\\Babydoll\\Produits\\Babybot-01\\Informatic-02\\StandUp\\venv2\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\dorva\\Documents\\GitHub\\dorvan\\Babydoll\\Produits\\Babybot-01\\Informatic-02\\StandUp\\venv2\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dorva\\Documents\\GitHub\\dorvan\\Babydoll\\Produits\\Babybot-01\\Informatic-02\\StandUp\\venv2\\Lib\\site-packages\\gymnasium\\core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dorva\\Documents\\GitHub\\dorvan\\Babydoll\\Produits\\Babybot-01\\Informatic-02\\StandUp\\venv2\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\dorvan\\Babydoll\\Produits\\Babybot-01\\Informatic-02\\StandUp\\standup_env\\standup_env\\envs\\standup.py:238\u001b[0m, in \u001b[0;36mStandUpEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    231\u001b[0m         p\u001b[38;5;241m.\u001b[39mresetDebugVisualizerCamera(\n\u001b[0;32m    232\u001b[0m         cameraDistance\u001b[38;5;241m=\u001b[39mcam[\u001b[38;5;241m10\u001b[39m],\n\u001b[0;32m    233\u001b[0m         cameraYaw\u001b[38;5;241m=\u001b[39mcam[\u001b[38;5;241m8\u001b[39m],\n\u001b[0;32m    234\u001b[0m         cameraPitch\u001b[38;5;241m=\u001b[39mcam[\u001b[38;5;241m9\u001b[39m],\n\u001b[0;32m    235\u001b[0m         cameraTargetPosition\u001b[38;5;241m=\u001b[39mrobot_position)\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;66;03m# Sleep\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_ \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minfo,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward:\u001b[39m\u001b[38;5;124m\"\u001b[39m: reward, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mangle_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m: angle_reward, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz_position_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m: z_position_reward}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Execute Setup\n",
    "import nbformat\n",
    "from IPython import get_ipython\n",
    "with open(\"01_standup.ipynb\", \"r\", encoding=\"utf-8\") as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "for cell in notebook.cells:\n",
    "    if \"tags\" in cell.metadata and \"setup\" in cell.metadata.tags:\n",
    "        exec(cell.source)\n",
    "\n",
    "env = make_env(render_mode='human')\n",
    "\n",
    "env.reset()\n",
    "\n",
    "terminated = False\n",
    "i=0\n",
    "while not terminated:\n",
    "    action = env.action_space.sample()\n",
    "    obs, rew, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Print the tobservations, reward\n",
    "    print(\"Step:\", i)\n",
    "    print(\"Action:\", action)\n",
    "    print(\"Observations:\", obs)\n",
    "    print(\"Reward:\", rew)\n",
    "    print(\"Info\", info)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    i+=1\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Setup\n",
    "import nbformat\n",
    "from IPython import get_ipython\n",
    "with open(\"01_standup.ipynb\", \"r\", encoding=\"utf-8\") as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "for cell in notebook.cells:\n",
    "    if \"tags\" in cell.metadata and \"setup\" in cell.metadata.tags:\n",
    "        exec(cell.source)\n",
    "\n",
    "# Linear Schedule\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        return progress_remaining * initial_value\n",
    "    return func\n",
    "\n",
    "# Create model\n",
    "env = make_env()\n",
    "\n",
    "\n",
    "if agent is DQN:\n",
    "    model = DQN(\n",
    "        policy,\n",
    "        env,\n",
    "        verbose=0,\n",
    "        device=device,\n",
    "        tensorboard_log=tensorboard_log,\n",
    "        exploration_fraction=0.5,\n",
    "        learning_rate=linear_schedule(0.0001)\n",
    "    )\n",
    "    \n",
    "elif agent is DDPG:\n",
    "    model = DDPG(\n",
    "        policy,\n",
    "        env,\n",
    "        verbose=0,\n",
    "        device=device,\n",
    "        tensorboard_log=tensorboard_log,\n",
    "        learning_rate=linear_schedule(0.0001)\n",
    "    )\n",
    "    \n",
    "elif agent is A2C:\n",
    "    model = A2C(\n",
    "        policy,\n",
    "        env,\n",
    "        verbose=0,\n",
    "        device=device,\n",
    "        tensorboard_log=tensorboard_log,    \n",
    "    )\n",
    "    \n",
    "elif agent is PPO:\n",
    "    model = PPO(\n",
    "        policy,\n",
    "        env,\n",
    "        verbose=0,\n",
    "        device=device,\n",
    "        tensorboard_log=tensorboard_log,    \n",
    "    )\n",
    "\n",
    "# Save\n",
    "shutil.rmtree(dir, ignore_errors=True)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Setup\n",
    "import nbformat\n",
    "from IPython import get_ipython\n",
    "with open(\"01_standup.ipynb\", \"r\", encoding=\"utf-8\") as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "for cell in notebook.cells:\n",
    "    if \"tags\" in cell.metadata and \"setup\" in cell.metadata.tags:\n",
    "        exec(cell.source)\n",
    "\n",
    "print(f\"Start training with {agent}\")\n",
    "# Env and model\n",
    "train_env = Monitor(make_env())\n",
    "eval_env = Monitor(make_env())\n",
    "\n",
    "model = agent.load(model_path, train_env, device)\n",
    "\n",
    "# Callbacks\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    eval_freq=1e4,\n",
    "    deterministic=True,\n",
    "    n_eval_episodes=10,\n",
    "    best_model_save_path=best_model_save_path,\n",
    ")\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    1e4,\n",
    "    best_model_save_path,\n",
    "    name_prefix=\"checkpoint\"\n",
    ")\n",
    "\n",
    "# Training\n",
    "model.learn(\n",
    "    total_timesteps=1e6,\n",
    "    progress_bar=True,\n",
    "    reset_num_timesteps=False,\n",
    "    \n",
    "    callback=[\n",
    "        eval_callback,\n",
    "        checkpoint_callback\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Save and close\n",
    "model.save(model_path)\n",
    "train_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
