{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import babybot01_env\n",
    "from tqdm import tqdm\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "'''\n",
    "Use DQN to reach the target.\n",
    "\n",
    " - Run env at 60 fps (under 60 it srcew up)\n",
    " - Set a sim_factor to 5 to make the env_freq 5x slower than the sim_freq\n",
    " - Expose 2 phases_biases (0,65)\n",
    " - Performe 6 discrete action (\n",
    "    - do noting, \n",
    "    - increase phases_biases 0\n",
    "    - decrease phases_biases 0\n",
    "    - increase phases_biases 65\n",
    "    - decrease phases_biases 65\n",
    "    )\n",
    " - Observe 3 continuous values \n",
    "    - phase_biases 0 [-1, 1]\n",
    "    - phase_biases 65 [-1, 1]\n",
    "    - alignement [-1,1]\n",
    "\n",
    "Task:\n",
    "    Map phase_biase 0 and 65 incrementation according to current phase_biases 0 and 65 and current alignment.\n",
    "     \n",
    "\n",
    "'''\n",
    "\n",
    "name = 'dqn_spidy_v4_4'\n",
    "env_id = \"Spidy-v4_4\"\n",
    "sim_frequence = 60\n",
    "sim_factor = 60\n",
    "n_steps = 60\n",
    "n_envs = 1\n",
    "exposed_phases_indexes = [0,65]\n",
    "action_mode = \"discrete\"\n",
    "\n",
    "policy = 'MlpPolicy'\n",
    "tensorboard_log = f\"./{name}/t_logs/\"\n",
    "save_path = f\"./{name}/model/\"\n",
    "path = f\"./{name}/model/{name}\"\n",
    "log_path = f\"./{name}/logs/\"\n",
    "device = 'cpu'\n",
    "\n",
    "def make_env(render_mode:str=None, debug_mode = False):\n",
    "    return gym.make(\n",
    "        env_id, \n",
    "        sim_frequence=sim_frequence,\n",
    "        sim_factor=sim_factor,\n",
    "        max_episode_steps=n_steps, \n",
    "        exposed_phases_indexes= exposed_phases_indexes, \n",
    "        render_mode=render_mode,\n",
    "        debug_mode= debug_mode,\n",
    "        action_mode=action_mode,\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "print(f\"First Obs: {obs}\")\n",
    "#print(f\"First Phases biases: {info['phase_biases']}\")\n",
    "\n",
    "action = [0.1, -0.1]\n",
    "obs, rew, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(f\"After action Obs: {obs}\")\n",
    "#print(f\"Second Phases biases: {info['phase_biases']}\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "print(f\"After reset Obs: {obs}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Setup\n",
    "import nbformat\n",
    "from IPython import get_ipython\n",
    "with open(\"06_reach_DQN.ipynb\", \"r\", encoding=\"utf-8\") as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "for cell in notebook.cells:\n",
    "    if \"tags\" in cell.metadata and \"setup\" in cell.metadata.tags:\n",
    "        exec(cell.source)\n",
    "\n",
    "# Create Env\n",
    "env = make_env(debug_mode=False, render_mode='human')\n",
    "\n",
    "# Run env\n",
    "\n",
    "obs, info = env.reset()\n",
    "for t in range(10):\n",
    "    \n",
    "    action = np.random.randint(0, 5)\n",
    "    obs, rew, terminated, truncated, info = env.step(action)\n",
    "    print(f\"obs: {obs}\")\n",
    "    print(f\"rew: {rew}\")\n",
    "    print(f\"t: {t}\")\n",
    "    print(f\"proximity_reward: {info['proximity_reward']}, alignement_reward: {info['alignement_reward']}\")\n",
    "    print(f\"terminate: {terminated}, truncated: {truncated}\")\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = make_env()\n",
    "\n",
    "# PPO\n",
    "# model = PPO(\n",
    "#     policy, \n",
    "#     train_env, \n",
    "#     batch_size = 30, \n",
    "#     verbose=0, \n",
    "#     n_steps=n_steps, \n",
    "#     tensorboard_log=tensorboard_log,\n",
    "#     ent_coef=0           \n",
    "# )\n",
    "\n",
    "model = DQN(\n",
    "    policy,\n",
    "    train_env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=tensorboard_log\n",
    "    )\n",
    "\n",
    "model.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Setup\n",
    "import nbformat\n",
    "from IPython import get_ipython\n",
    "with open(\"06_reach_DQN.ipynb\", \"r\", encoding=\"utf-8\") as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "for cell in notebook.cells:\n",
    "    if \"tags\" in cell.metadata and \"setup\" in cell.metadata.tags:\n",
    "        exec(cell.source)\n",
    "\n",
    "class ActionLoggerCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(ActionLoggerCallback, self).__init__(verbose)\n",
    "        self.writer = None\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        # Initialize TensorBoard writer\n",
    "        self.writer = SummaryWriter(log_dir=self.logger.dir)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Get the actions from the rollout buffer\n",
    "        actions = self.locals['actions']  # PPO stores actions here\n",
    "        if actions is not None:\n",
    "            actions_mean = np.mean(actions)\n",
    "            actions_std = np.std(actions)\n",
    "            self.writer.add_scalar(\"policy/actions_mean\", actions_mean, self.num_timesteps)\n",
    "            self.writer.add_scalar(\"policy/actions_std\", actions_std, self.num_timesteps)\n",
    "        return True\n",
    "\n",
    "class InfoCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.log_freq = 1\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Access the training environment's 'step_info' attribute\n",
    "        if self.num_timesteps % self.log_freq == 0:\n",
    "            info = self.locals['infos']\n",
    "            info = info[0]\n",
    "            \n",
    "            self.logger.record(\"exposed_phases_biases/phase_0\", info[\"exposed_phases_biases\"][0])\n",
    "            self.logger.record(\"exposed_phases_biases/phase_65\", info[\"exposed_phases_biases\"][1])\n",
    "            self.logger.record(\"rewards/proximity_reward\", info[\"proximity_reward\"])\n",
    "            self.logger.record(\"rewards/alignement_reward\", info[\"alignement_reward\"])\n",
    "\n",
    "            actions = self.locals[\"actions\"][0]\n",
    "            self.logger.record(\"actions\", actions)\n",
    "\n",
    "            self.logger.dump(step=self.num_timesteps)\n",
    "        \n",
    "        return True\n",
    "            \n",
    "train_env = Monitor(make_env())\n",
    "eval_env = Monitor(make_env())\n",
    "\n",
    "eval_callback = EvalCallback(eval_env,\n",
    "                             best_model_save_path=save_path,\n",
    "                             log_path=log_path, eval_freq=1e3,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "class SaveOnStep(BaseCallback):\n",
    "    def __init__(self, steps: int, path: str):\n",
    "        super().__init__()\n",
    "        self.steps = steps\n",
    "        self.save_path = path\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Check if the current step matches the saving frequency\n",
    "        if self.n_calls % self.steps == 0:\n",
    "            # Save model with the current timestep in the filename\n",
    "\n",
    "            print(f\"Saving model at step {self.n_calls} to {self.save_path}\")\n",
    "            self.model.save(self.save_path)\n",
    "        return True\n",
    "    \n",
    "callbacks = [\n",
    "    SaveOnStep(1e3, path), \n",
    "    InfoCallback(), \n",
    "    ActionLoggerCallback(),\n",
    "    eval_callback\n",
    "    ]\n",
    "\n",
    "model = DQN.load(path,train_env ,device=device)\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=1e5,\n",
    "    progress_bar=True, \n",
    "    callback=callbacks, \n",
    "    reset_num_timesteps=False)\n",
    "\n",
    "model.save(path+\"_final\")\n",
    "\n",
    "train_env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Setup\n",
    "import nbformat\n",
    "from IPython import get_ipython\n",
    "with open(\"06_reach_DQN.ipynb\", \"r\", encoding=\"utf-8\") as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "for cell in notebook.cells:\n",
    "    if \"tags\" in cell.metadata and \"setup\" in cell.metadata.tags:\n",
    "        exec(cell.source)\n",
    "\n",
    "test_env = make_env(render_mode='human')\n",
    "model = DQN.load(path)\n",
    "info = {}\n",
    "\n",
    "for episode in range(1):\n",
    "\n",
    "    done = False\n",
    "    obs, info = test_env.reset()\n",
    "    for t in range(n_steps):\n",
    "\n",
    "        action = model.predict(obs)[0]\n",
    "        obs, reward, terminate, trunc, info = test_env.step(action)\n",
    "    \n",
    "        if terminate or trunc:\n",
    "            break\n",
    "\n",
    "        print(f\"t: {t}, Obs: {np.array_str(obs, precision=2)}, Action: {np.array_str(action, precision=2)}, Rew: {reward:.2f} \")\n",
    "\n",
    "test_env.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show CPG parameters\n",
    "coupling_weights = info['coupling_weights']\n",
    "phase_biases= info['phase_biases']\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2,1, figsize=(12, 6))  # Adjust size if needed\n",
    "\n",
    "im1 = axs[0].imshow(coupling_weights, cmap='viridis', aspect='equal')\n",
    "fig.colorbar(im1, ax=axs[0], orientation='vertical')  # Add a color bar for reference\n",
    "axs[0].set_title(\"coupling_weights\")\n",
    "axs[0].set_xlabel(\"Column Index\")\n",
    "axs[0].set_ylabel(\"Row Index\")\n",
    "axs[0].set_xticks(range(12))\n",
    "axs[0].set_yticks(range(12))\n",
    "\n",
    "im2 = axs[1].imshow(phase_biases, cmap='viridis', aspect='equal')\n",
    "fig.colorbar(im1, ax=axs[1], orientation='vertical')  # Add a color bar for reference\n",
    "axs[1].set_title(\"phase_biases\")\n",
    "axs[1].set_xlabel(\"Column Index\")\n",
    "axs[1].set_ylabel(\"Row Index\")\n",
    "axs[1].set_xticks(range(12))\n",
    "axs[1].set_yticks(range(12))\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hostEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
